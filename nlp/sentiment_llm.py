import os
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Optional, Dict, List
import openai
import redis
from functools import lru_cache
from health.metrics import record

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_TEMP = float(os.getenv("LLM_TEMP", "0.0"))
LLM_MAXTOK = int(os.getenv("LLM_MAXTOK", "8"))
CACHE_HOURS = int(os.getenv("CACHE_HOURS", "24"))
LLM_OFF = bool(int(os.getenv("LLM_OFF", "0")))

# Redis –∫–ª–∏–µ–Ω—Ç (—Å fallback –Ω–∞ in-memory —Å–ª–æ–≤–∞—Ä—å)
try:
    redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    redis_client.ping()
    print("‚úÖ Redis –ø–æ–¥–∫–ª—é—á–µ–Ω")
except:
    pass  # –¢–∏—Ö–æ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ in-memory –∫—ç—à
    redis_client = {}

# SQLite –±–∞–∑–∞ –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
DB_PATH = "news_cache.db"

def init_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç SQLite –±–∞–∑—É –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS sentiment_cache (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            text_hash TEXT UNIQUE NOT NULL,
            text TEXT NOT NULL,
            sentiment TEXT NOT NULL,
            confidence REAL DEFAULT 0.5,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            ticker TEXT,
            source TEXT DEFAULT 'llm'
        )
    ''')

    # –ò–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ç–∏–∫–µ—Ä—É
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_timestamp_ticker 
        ON sentiment_cache(timestamp, ticker)
    ''')

    conn.commit()
    conn.close()

def build_prompt(text: str) -> Dict[str, str]:
    """–°—Ç—Ä–æ–∏—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM"""
    system_prompt = "Classify financial news sentiment: positive/negative/neutral. Reply with one word only."

    # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
    user_text = text[:100] + "..." if len(text) > 100 else text

    return {
        "system": system_prompt,
        "user": f"Text: {user_text}"
    }

def call_openai_sync(prompt: Dict[str, str], max_tokens: int = LLM_MAXTOK, temperature: float = LLM_TEMP) -> str:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI API"""
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

    if LLM_OFF:
        raise ValueError("LLM –∞–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª—é—á–µ–Ω (LLM_OFF=1)")

    client = openai.OpenAI(api_key=OPENAI_API_KEY)

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # –ë–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å
            messages=[
                {"role": "system", "content": prompt["system"]},
                {"role": "user", "content": prompt["user"]}
            ],
            max_tokens=max_tokens,
            temperature=temperature,
            timeout=10
        )

        usage = response.usage
        record("gpt_tokens", {
            "model":        response.model,
            "prompt":       usage.prompt_tokens,
            "completion":   usage.completion_tokens,
            "total":        usage.total_tokens
        })

        result = response.choices[0].message.content.strip().lower()

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–≤–µ—Ç
        if "positive" in result:
            return "positive"
        elif "negative" in result:
            return "negative"
        else:
            return "neutral"

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI API: {e}")
        return "neutral"  # Fallback

def get_text_hash(text: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö—ç—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    import hashlib
    return hashlib.md5(text.encode()).hexdigest()

def cache_get(text_hash: str) -> Optional[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ (Redis + SQLite)"""
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º Redis
    if isinstance(redis_client, dict):
        # In-memory fallback
        cached = redis_client.get(text_hash)
    else:
        try:
            cached = redis_client.get(f"sentiment:{text_hash}")
        except:
            cached = None

    if cached:
        try:
            return json.loads(cached)
        except:
            pass

    # –ï—Å–ª–∏ Redis –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–≤–µ—Ä—è–µ–º SQLite
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # –ò—â–µ–º —Å–≤–µ–∂–∏–µ –∑–∞–ø–∏—Å–∏ (–Ω–µ —Å—Ç–∞—Ä—à–µ CACHE_HOURS)
    cutoff = datetime.now() - timedelta(hours=CACHE_HOURS)

    cursor.execute('''
        SELECT sentiment, confidence, timestamp 
        FROM sentiment_cache 
        WHERE text_hash = ? AND timestamp > ?
        ORDER BY timestamp DESC LIMIT 1
    ''', (text_hash, cutoff.isoformat()))

    result = cursor.fetchone()
    conn.close()

    if result:
        return {
            "sentiment": result[0],
            "confidence": result[1],
            "timestamp": result[2]
        }

    return None

def cache_set(text_hash: str, text: str, sentiment: str, confidence: float = 0.5, ticker: str = None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à (Redis + SQLite)"""
    data = {
        "sentiment": sentiment,
        "confidence": confidence,
        "timestamp": datetime.now().isoformat()
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Redis
    if isinstance(redis_client, dict):
        # In-memory fallback
        redis_client[text_hash] = json.dumps(data)
    else:
        try:
            redis_client.setex(
                f"sentiment:{text_hash}", 
                timedelta(hours=CACHE_HOURS), 
                json.dumps(data)
            )
        except:
            pass

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ SQLite
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    try:
        cursor.execute('''
            INSERT OR REPLACE INTO sentiment_cache 
            (text_hash, text, sentiment, confidence, ticker, source)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (text_hash, text, sentiment, confidence, ticker, 'llm'))

        conn.commit()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ SQLite: {e}")
    finally:
        conn.close()

def smart_classify(text: str, ticker: str = None) -> str:
    """–£–º–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not text or not text.strip():
        return "neutral"

    text_hash = get_text_hash(text.strip())

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached = cache_get(text_hash)
    if cached:
        print(f"üîÑ –ö—ç—à: {cached['sentiment']}")
        return cached["sentiment"]

    # Fallback –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    if LLM_OFF or not OPENAI_API_KEY:
        sentiment = fallback_classify(text)
        cache_set(text_hash, text, sentiment, 0.3, ticker)
        return sentiment

    # –í—ã–∑—ã–≤–∞–µ–º LLM
    try:
        prompt = build_prompt(text)
        sentiment = call_openai_sync(prompt)

        print(f"ü§ñ LLM: {sentiment}")
        cache_set(text_hash, text, sentiment, 0.8, ticker)
        return sentiment

    except Exception as e:
        print(f"‚ùå LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        sentiment = fallback_classify(text)
        cache_set(text_hash, text, sentiment, 0.3, ticker)
        return sentiment

def fallback_classify(text: str) -> str:
    """Fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    text_lower = text.lower()

    positive_words = [
        '—Ä–æ—Å—Ç', '–ø—Ä–∏–±—ã–ª—å', '–¥–æ—Ö–æ–¥', '–≤—ã—Ä–æ—Å–ª–∏', '—É–≤–µ–ª–∏—á', '–ø–æ–≤—ã—à', '—É–ª—É—á—à',
        'rise', 'gain', 'profit', 'increase', 'growth', 'up', 'strong', 'beat'
    ]

    negative_words = [
        '–ø–∞–¥–µ–Ω–∏–µ', '—É–±—ã—Ç–æ–∫', '—Å–Ω–∏–∑–∏–ª', '—É–ø–∞–ª–∏', '–∫—Ä–∏–∑–∏—Å', '—É–º–µ–Ω—å—à', '–ø–æ—Ç–µ—Ä',
        'decline', 'loss', 'drop', 'fall', 'down', 'weak', 'miss', 'disappoint'
    ]

    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)

    if pos_count > neg_count:
        return "positive"
    elif neg_count > pos_count:
        return "negative"
    else:
        return "neutral"

def get_sentiment_score_from_cache(ticker: str, hours: int = 24, force_refresh: bool = False) -> int:
    """–ü–æ–ª—É—á–∏—Ç—å –æ—Ü–µ–Ω–∫—É –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞ SQLite, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–≤–µ–∂–∏–µ"""

    # –ï—Å–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫—ç—à
    if force_refresh:
        print(f"üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è {ticker}")
        return 0

    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        # –ò—â–µ–º –∑–∞–ø–∏—Å–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
        cutoff = datetime.now() - timedelta(hours=hours)

        cursor.execute('''
            SELECT sentiment FROM sentiment_cache 
            WHERE timestamp > ? AND (ticker = ? OR text LIKE ?)
            ORDER BY timestamp DESC
        ''', (cutoff.isoformat(), ticker, f'%{ticker}%'))

        results = cursor.fetchall()
        conn.close()

        if not results:
            print(f"üìä –ö—ç—à –ø—É—Å—Ç –¥–ª—è {ticker}")
            return 0

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º score
        score = 0
        for (sentiment,) in results:
            if sentiment == "positive":
                score += 1
            elif sentiment == "negative":
                score -= 1

        print(f"üìä –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ {ticker}: {score} (–∏–∑ {len(results)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –∫—ç—à–µ)")
        return score

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ score –∏–∑ –∫—ç—à–∞: {e}")
        return 0

def get_cache_stats() -> Dict:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
    cursor.execute("SELECT COUNT(*) FROM sentiment_cache")
    total = cursor.fetchone()[0]

    # –ó–∞–ø–∏—Å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
    cutoff = datetime.now() - timedelta(hours=24)
    cursor.execute("SELECT COUNT(*) FROM sentiment_cache WHERE timestamp > ?", (cutoff.isoformat(),))
    recent = cursor.fetchone()[0]

    # –ó–∞–ø–∏—Å–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    cursor.execute("SELECT source, COUNT(*) FROM sentiment_cache GROUP BY source")
    by_source = dict(cursor.fetchall())

    conn.close()

    return {
        "total_entries": total,
        "recent_24h": recent,
        "by_source": by_source
    }

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
init_database()
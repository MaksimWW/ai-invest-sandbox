
from functools import lru_cache
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch, requests, datetime as dt
from news_feed import fetch_news
from langdetect import detect
import warnings

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤–µ—Å–∞—Ö –º–æ–¥–µ–ª–∏
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è)
RU_MODELS = [
    {
        "name": "seara/rubert-base-cased-russian-sentiment",
        "labels": ["NEGATIVE", "NEUTRAL", "POSITIVE"]
    },
    {
        "name": "sismetanin/rubert-ru-sentiment-rusentiment",
        "labels": ["NEGATIVE", "NEUTRAL", "POSITIVE"]
    },
    {
        "name": "blanchefort/rubert-base-cased-sentiment",
        "labels": ["NEUTRAL", "POSITIVE", "NEGATIVE"]
    }
]

# –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
EN_MODEL_NAME = "cardiffnlp/twitter-roberta-base-sentiment-latest"
EN_LABELS = ["LABEL_0", "LABEL_1", "LABEL_2"]  # negative, neutral, positive

@lru_cache(maxsize=2)
def _load_models():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏"""
    ru_tok, ru_mdl, ru_labels = None, None, None
    
    # –ü—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏
    for model_info in RU_MODELS:
        try:
            print(f"üîÑ –ü—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å: {model_info['name']}")
            ru_tok = AutoTokenizer.from_pretrained(model_info["name"])
            ru_mdl = AutoModelForSequenceClassification.from_pretrained(model_info["name"])
            ru_mdl.eval()
            ru_labels = model_info["labels"]
            print(f"‚úÖ –†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_info['name']}")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_info['name']}: {e}")
            continue
    
    if ru_tok is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å")

    try:
        # –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å...")
        en_tok = AutoTokenizer.from_pretrained(EN_MODEL_NAME)
        en_mdl = AutoModelForSequenceClassification.from_pretrained(EN_MODEL_NAME)
        en_mdl.eval()
        print("‚úÖ –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏: {e}")
        en_tok, en_mdl = None, None

    return ru_tok, ru_mdl, ru_labels, en_tok, en_mdl

def _normalize_sentiment(label: str, model_type: str = "ru") -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–µ—Ç–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
    if model_type == "ru":
        # –î–ª—è —Ä—É—Å—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
        label_upper = label.upper()
        if "NEGATIVE" in label_upper or "NEG" in label_upper:
            return "negative"
        elif "POSITIVE" in label_upper or "POS" in label_upper:
            return "positive"
        else:
            return "neutral"
    else:
        # –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏ (Twitter RoBERTa)
        mapping = {
            "LABEL_0": "negative",  # negative
            "LABEL_1": "neutral",   # neutral
            "LABEL_2": "positive"   # positive
        }
        return mapping.get(label, "neutral")

def _rule_based_sentiment_ru(text: str) -> str:
    """–ü—Ä–æ—Å—Ç–æ–π rule-based –∞–Ω–∞–ª–∏–∑ –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç"""
    text_lower = text.lower()
    
    positive_words = [
        "—Ä–æ—Å—Ç", "–ø—Ä–∏–±—ã–ª—å", "—É—Å–ø–µ—Ö", "—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ", "—Ä–µ–∫–æ—Ä–¥", "–≤—ã–∏–≥—Ä", 
        "–ø–ª—é—Å", "–ø–æ–≤—ã—à", "—É–≤–µ–ª–∏—á", "—É–ª—É—á—à", "–ø–æ–∑–∏—Ç–∏–≤", "–≤—ã–≥–æ–¥", "–¥–æ—Ö–æ–¥"
    ]
    
    negative_words = [
        "–ø–∞–¥–µ–Ω–∏–µ", "—É–±—ã—Ç–æ–∫", "–∫—Ä–∏–∑–∏—Å", "–ø–ª–æ—Ö–æ", "—É–∂–∞—Å–Ω–æ", "–ø—Ä–æ–≤–∞–ª", "–ø—Ä–æ–∏–≥—Ä",
        "–º–∏–Ω—É—Å", "—Å–Ω–∏–∑–∏–ª", "—É–º–µ–Ω—å—à", "—É—Ö—É–¥—à", "–Ω–µ–≥–∞—Ç–∏–≤", "–ø–æ—Ç–µ—Ä", "–¥–æ–ª–≥"
    ]
    
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)
    
    if pos_count > neg_count:
        return "positive"
    elif neg_count > pos_count:
        return "negative"
    else:
        return "neutral"

def classify_ru(text: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
    try:
        ru_tok, ru_mdl, ru_labels, _, _ = _load_models()
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
        if ru_tok is not None and ru_mdl is not None:
            inputs = ru_tok(text, return_tensors="pt", truncation=True, max_length=512)
            with torch.no_grad():
                logits = ru_mdl(**inputs).logits
            
            probabilities = torch.softmax(logits, dim=-1)
            predicted_idx = logits.argmax().item()
            predicted_label = ru_labels[predicted_idx]
            confidence = probabilities[0][predicted_idx].item()
            
            print(f"üîç RU MODEL: '{text[:50]}...'")
            print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {[f'{ru_labels[i]}={probabilities[0][i]:.3f}' for i in range(len(ru_labels))]}")
            print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
            
            result = _normalize_sentiment(predicted_label, "ru")
            
            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based
            if confidence < 0.7:
                rule_result = _rule_based_sentiment_ru(text)
                print(f"‚ö° Rule-based —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {rule_result}")
                if rule_result != "neutral":
                    result = rule_result
                    print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º rule-based —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            
            print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            return result
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based
        else:
            print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º rule-based –∞–Ω–∞–ª–∏–∑ –¥–ª—è: '{text[:50]}...'")
            result = _rule_based_sentiment_ru(text)
            print(f"‚úÖ Rule-based —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            return result
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π rule-based –∞–Ω–∞–ª–∏–∑
        return _rule_based_sentiment_ru(text)

def classify_en(text: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    try:
        _, _, _, en_tok, en_mdl = _load_models()
        if en_tok is None or en_mdl is None:
            return "neutral"
        
        inputs = en_tok(text, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            logits = en_mdl(**inputs).logits
        
        probabilities = torch.softmax(logits, dim=-1)
        predicted_idx = logits.argmax().item()
        predicted_label = EN_LABELS[predicted_idx]
        confidence = probabilities[0][predicted_idx].item()
        
        print(f"üîç EN DEBUG: '{text[:50]}...'")
        print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {[f'{EN_LABELS[i]}={probabilities[0][i]:.3f}' for i in range(len(EN_LABELS))]}")
        print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
        
        result = _normalize_sentiment(predicted_label, "en")
        print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        return result
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
        return "neutral"

def classify_multi(text: str) -> str:
    """–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è"""
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
        lang = detect(text[:200])
        if lang == "ru":
            return classify_ru(text)
        else:
            return classify_en(text)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞: {e}")
        # –ï—Å–ª–∏ —è–∑—ã–∫ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª—Å—è, –ø—Ä–æ–±—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
        return classify_en(text)

# –°—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def classify(text: str) -> str:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - –∞–Ω–∞–ª–∏–∑ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    return classify_ru(text)

# --- RSS-–≥—Ä–∞–±–±–µ—Ä —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ ---
RSS_FEEDS = [
    "https://tass.ru/rss/v2.xml",
    "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
]

def latest_news_ru(ticker: str, hours: int = 24) -> list[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä—É—Å—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS"""
    cutoff = dt.datetime.utcnow() - dt.timedelta(hours=hours)
    found = []
    for url in RSS_FEEDS:
        try:
            xml = requests.get(url, timeout=5).text
            for it in xml.split("<item>")[1:]:
                title = it.split("<title>")[1].split("</title>")[0]
                pub   = it.split("<pubDate>")[1].split("</pubDate>")[0]
                dt_pub = dt.datetime.strptime(pub[:-6], "%a, %d %b %Y %H:%M:%S")
                if dt_pub > cutoff and ticker.lower() in title.lower():
                    found.append(title)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS {url}: {e}")
            continue
    return found

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def latest_news(ticker: str, hours: int = 24) -> list[str]:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - —Ä—É—Å—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    return latest_news_ru(ticker, hours)

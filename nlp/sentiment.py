
from functools import lru_cache
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch, requests, datetime as dt, re
from news_feed import fetch_news
from langdetect import detect
import warnings
from typing import Dict, List, Tuple

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤–µ—Å–∞—Ö –º–æ–¥–µ–ª–∏
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è)
RU_MODELS = [
    {
        "name": "seara/rubert-base-cased-russian-sentiment",
        "labels": ["NEGATIVE", "NEUTRAL", "POSITIVE"]
    },
    {
        "name": "sismetanin/rubert-ru-sentiment-rusentiment",
        "labels": ["NEGATIVE", "NEUTRAL", "POSITIVE"]
    },
    {
        "name": "blanchefort/rubert-base-cased-sentiment",
        "labels": ["NEUTRAL", "POSITIVE", "NEGATIVE"]
    }
]

# –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
EN_MODEL_NAME = "cardiffnlp/twitter-roberta-base-sentiment-latest"
EN_LABELS = ["LABEL_0", "LABEL_1", "LABEL_2"]  # negative, neutral, positive

class ContextualSentimentAnalyzer:
    """–ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self.magnitude_patterns = {
            'high_positive': r'(—Ä–µ–∫–æ—Ä–¥|–≤–∑–ª–µ—Ç|—Å–∫–∞—á–æ–∫|–±—É–º|–ø—Ä–µ–≤–∑–æ—à|breakthrough|surge)',
            'moderate_positive': r'(–≤—ã—Ä–æ—Å–ª–∏|—Ä–æ—Å—Ç|—É–≤–µ–ª–∏—á|–ø–æ–≤—ã—à|—É–ª—É—á—à|improved|gained)',
            'high_negative': r'(–æ–±–≤–∞–ª|–∫—Ä–∞—Ö|–∫—Ä–∏–∑–∏—Å|–∫–æ–ª–ª–∞–ø—Å|plummet|crash|collapse)',
            'moderate_negative': r'(—É–ø–∞–ª–∏|—Å–Ω–∏–∑–∏–ª|–ø–∞–¥–µ–Ω–∏–µ|—É–º–µ–Ω—å—à|declined|dropped)',
            'neutral_stable': r'(—Å—Ç–∞–±–∏–ª—å–Ω|–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω|–æ—Å—Ç–∞–ª|remained|stable|flat)'
        }
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        self.number_pattern = r'(\d+(?:,\d+)?(?:\.\d+)?)\s*%'
        
    def extract_magnitude(self, text: str) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–µ–ª–∏—á–∏–Ω–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        text_lower = text.lower()
        
        # –ù–∞—Ö–æ–¥–∏–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        numbers = re.findall(self.number_pattern, text_lower)
        max_number = 0
        if numbers:
            try:
                max_number = max(float(num.replace(',', '.')) for num in numbers)
            except:
                max_number = 0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        magnitude_scores = {}
        for mag_type, pattern in self.magnitude_patterns.items():
            matches = len(re.findall(pattern, text_lower))
            magnitude_scores[mag_type] = matches
            
        return {
            'numeric_value': max_number,
            'magnitude_scores': magnitude_scores,
            'has_strong_signals': any(score > 0 for key, score in magnitude_scores.items() 
                                    if 'high_' in key)
        }
    
    def calculate_contextual_score(self, text: str, ml_result: str, ml_confidence: float) -> Tuple[str, float]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        magnitude_info = self.extract_magnitude(text)
        
        # –ë–∞–∑–æ–≤—ã–µ –æ—á–∫–∏ –æ—Ç ML
        ml_score = {
            'positive': 1,
            'negative': -1, 
            'neutral': 0
        }.get(ml_result, 0)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–ª–∏—á–∏–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏–π
        numeric_bonus = 0
        if magnitude_info['numeric_value'] > 0:
            if magnitude_info['numeric_value'] >= 10:  # –ë–æ–ª—å—à–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                numeric_bonus = 0.5 if ml_score > 0 else -0.5
            elif magnitude_info['numeric_value'] >= 5:  # –°—Ä–µ–¥–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                numeric_bonus = 0.3 if ml_score > 0 else -0.3
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ —Å–∏–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
        strong_signal_bonus = 0
        if magnitude_info['has_strong_signals']:
            if magnitude_info['magnitude_scores'].get('high_positive', 0) > 0:
                strong_signal_bonus = 0.7
            elif magnitude_info['magnitude_scores'].get('high_negative', 0) > 0:
                strong_signal_bonus = -0.7
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –∫ –Ω–µ–≥–∞—Ç–∏–≤—É
        bias_correction = 0
        if ml_result == 'negative' and ml_confidence < 0.7:
            # –ï—Å–ª–∏ ML —Å–ª–∞–±–æ —É–≤–µ—Ä–µ–Ω –≤ –Ω–µ–≥–∞—Ç–∏–≤–µ, –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
            if magnitude_info['magnitude_scores'].get('neutral_stable', 0) > 0:
                bias_correction = 0.5  # –°–¥–≤–∏–≥–∞–µ–º –∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–º—É
        
        # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä
        final_score = ml_score + numeric_bonus + strong_signal_bonus + bias_correction
        final_confidence = min(1.0, ml_confidence + abs(numeric_bonus) + abs(strong_signal_bonus))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if final_score > 0.3:
            result = 'positive'
        elif final_score < -0.3:
            result = 'negative'
        else:
            result = 'neutral'
            
        return result, final_confidence

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
_context_analyzer = ContextualSentimentAnalyzer()

@lru_cache(maxsize=2)
def _load_models():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏"""
    ru_tok, ru_mdl, ru_labels = None, None, None
    
    # –ü—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏
    for model_info in RU_MODELS:
        try:
            print(f"üîÑ –ü—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å: {model_info['name']}")
            ru_tok = AutoTokenizer.from_pretrained(model_info["name"])
            ru_mdl = AutoModelForSequenceClassification.from_pretrained(model_info["name"])
            ru_mdl.eval()
            ru_labels = model_info["labels"]
            print(f"‚úÖ –†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_info['name']}")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_info['name']}: {e}")
            continue
    
    if ru_tok is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å")

    try:
        # –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å...")
        en_tok = AutoTokenizer.from_pretrained(EN_MODEL_NAME)
        en_mdl = AutoModelForSequenceClassification.from_pretrained(EN_MODEL_NAME)
        en_mdl.eval()
        print("‚úÖ –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏: {e}")
        en_tok, en_mdl = None, None

    return ru_tok, ru_mdl, ru_labels, en_tok, en_mdl

def _normalize_sentiment(label: str, model_type: str = "ru") -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–µ—Ç–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
    if model_type == "ru":
        # –î–ª—è —Ä—É—Å—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
        label_upper = label.upper()
        if "NEGATIVE" in label_upper or "NEG" in label_upper:
            return "negative"
        elif "POSITIVE" in label_upper or "POS" in label_upper:
            return "positive"
        else:
            return "neutral"
    else:
        # –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏ (Twitter RoBERTa)
        mapping = {
            "LABEL_0": "negative",  # negative
            "LABEL_1": "neutral",   # neutral
            "LABEL_2": "positive"   # positive
        }
        return mapping.get(label, "neutral")

def classify_ru(text: str) -> str:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º"""
    try:
        ru_tok, ru_mdl, ru_labels, _, _ = _load_models()
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
        if ru_tok is not None and ru_mdl is not None:
            inputs = ru_tok(text, return_tensors="pt", truncation=True, max_length=512)
            with torch.no_grad():
                logits = ru_mdl(**inputs).logits
            
            probabilities = torch.softmax(logits, dim=-1)
            predicted_idx = logits.argmax().item()
            predicted_label = ru_labels[predicted_idx]
            confidence = probabilities[0][predicted_idx].item()
            
            print(f"üîç –ê–ù–ê–õ–ò–ó: '{text[:50]}...'")
            print(f"üìä ML –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {[f'{ru_labels[i]}={probabilities[0][i]:.3f}' for i in range(len(ru_labels))]}")
            print(f"üéØ ML –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
            
            ml_result = _normalize_sentiment(predicted_label, "ru")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            final_result, final_confidence = _context_analyzer.calculate_contextual_score(
                text, ml_result, confidence
            )
            
            print(f"üß† –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è: {ml_result} ‚Üí {final_result} (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {final_confidence:.3f})")
            print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {final_result}")
            
            return final_result
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        else:
            print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è: '{text[:50]}...'")
            return "neutral"
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return "neutral"

def classify_en(text: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º"""
    try:
        _, _, _, en_tok, en_mdl = _load_models()
        if en_tok is None or en_mdl is None:
            return "neutral"
        
        inputs = en_tok(text, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            logits = en_mdl(**inputs).logits
        
        probabilities = torch.softmax(logits, dim=-1)
        predicted_idx = logits.argmax().item()
        predicted_label = EN_LABELS[predicted_idx]
        confidence = probabilities[0][predicted_idx].item()
        
        print(f"üîç EN –ê–ù–ê–õ–ò–ó: '{text[:50]}...'")
        print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {[f'{EN_LABELS[i]}={probabilities[0][i]:.3f}' for i in range(len(EN_LABELS))]}")
        print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
        
        ml_result = _normalize_sentiment(predicted_label, "en")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        final_result, final_confidence = _context_analyzer.calculate_contextual_score(
            text, ml_result, confidence
        )
        
        print(f"üß† –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è: {ml_result} ‚Üí {final_result}")
        print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {final_result}")
        
        return final_result
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
        return "neutral"

def classify_multi(text: str) -> str:
    """–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è"""
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
        lang = detect(text[:200])
        if lang == "ru":
            return classify_ru(text)
        else:
            return classify_en(text)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞: {e}")
        # –ï—Å–ª–∏ —è–∑—ã–∫ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª—Å—è, –ø—Ä–æ–±—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
        return classify_en(text)

def analyze_sentiment_trend(texts: List[str]) -> Dict[str, float]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É —Ç–µ–∫—Å—Ç–æ–≤"""
    if not texts:
        return {'trend': 0.0, 'confidence': 0.0, 'count': 0}
    
    sentiments = []
    for text in texts:
        sentiment = classify_multi(text)
        score = {'positive': 1, 'negative': -1, 'neutral': 0}.get(sentiment, 0)
        sentiments.append(score)
    
    avg_sentiment = sum(sentiments) / len(sentiments)
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
    consistency = 1.0 - (len(set(sentiments)) - 1) / 2.0  # –æ—Ç 0 –¥–æ 1
    
    return {
        'trend': avg_sentiment,
        'confidence': consistency,
        'count': len(texts),
        'distribution': {
            'positive': sentiments.count(1),
            'negative': sentiments.count(-1),
            'neutral': sentiments.count(0)
        }
    }

# –°—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def classify(text: str) -> str:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - –∞–Ω–∞–ª–∏–∑ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    return classify_ru(text)

# --- RSS-–≥—Ä–∞–±–±–µ—Ä —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ ---
RSS_FEEDS = [
    "https://tass.ru/rss/v2.xml",
    "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
]

def latest_news_ru(ticker: str, hours: int = 24) -> list[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä—É—Å—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS"""
    cutoff = dt.datetime.utcnow() - dt.timedelta(hours=hours)
    found = []
    for url in RSS_FEEDS:
        try:
            xml = requests.get(url, timeout=5).text
            for it in xml.split("<item>")[1:]:
                title = it.split("<title>")[1].split("</title>")[0]
                pub   = it.split("<pubDate>")[1].split("</pubDate>")[0]
                dt_pub = dt.datetime.strptime(pub[:-6], "%a, %d %b %Y %H:%M:%S")
                if dt_pub > cutoff and ticker.lower() in title.lower():
                    found.append(title)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS {url}: {e}")
            continue
    return found

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def latest_news(ticker: str, hours: int = 24) -> list[str]:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - —Ä—É—Å—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    return latest_news_ru(ticker, hours)

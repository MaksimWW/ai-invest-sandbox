
from functools import lru_cache
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch, requests, datetime as dt
from news_feed import fetch_news
from langdetect import detect
import warnings

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤–µ—Å–∞—Ö –º–æ–¥–µ–ª–∏
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è)
RU_MODELS = [
    {
        "name": "seara/rubert-base-cased-russian-sentiment",
        "labels": ["NEGATIVE", "NEUTRAL", "POSITIVE"]
    },
    {
        "name": "sismetanin/rubert-ru-sentiment-rusentiment",
        "labels": ["NEGATIVE", "NEUTRAL", "POSITIVE"]
    },
    {
        "name": "blanchefort/rubert-base-cased-sentiment",
        "labels": ["NEUTRAL", "POSITIVE", "NEGATIVE"]
    }
]

# –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
EN_MODEL_NAME = "cardiffnlp/twitter-roberta-base-sentiment-latest"
EN_LABELS = ["LABEL_0", "LABEL_1", "LABEL_2"]  # negative, neutral, positive

@lru_cache(maxsize=2)
def _load_models():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏"""
    ru_tok, ru_mdl, ru_labels = None, None, None
    
    # –ü—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏
    for model_info in RU_MODELS:
        try:
            print(f"üîÑ –ü—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å: {model_info['name']}")
            ru_tok = AutoTokenizer.from_pretrained(model_info["name"])
            ru_mdl = AutoModelForSequenceClassification.from_pretrained(model_info["name"])
            ru_mdl.eval()
            ru_labels = model_info["labels"]
            print(f"‚úÖ –†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_info['name']}")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_info['name']}: {e}")
            continue
    
    if ru_tok is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å")

    try:
        # –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å...")
        en_tok = AutoTokenizer.from_pretrained(EN_MODEL_NAME)
        en_mdl = AutoModelForSequenceClassification.from_pretrained(EN_MODEL_NAME)
        en_mdl.eval()
        print("‚úÖ –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏: {e}")
        en_tok, en_mdl = None, None

    return ru_tok, ru_mdl, ru_labels, en_tok, en_mdl

def _normalize_sentiment(label: str, model_type: str = "ru") -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–µ—Ç–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
    if model_type == "ru":
        # –î–ª—è —Ä—É—Å—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
        label_upper = label.upper()
        if "NEGATIVE" in label_upper or "NEG" in label_upper:
            return "negative"
        elif "POSITIVE" in label_upper or "POS" in label_upper:
            return "positive"
        else:
            return "neutral"
    else:
        # –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏ (Twitter RoBERTa)
        mapping = {
            "LABEL_0": "negative",  # negative
            "LABEL_1": "neutral",   # neutral
            "LABEL_2": "positive"   # positive
        }
        return mapping.get(label, "neutral")

def _rule_based_sentiment_ru(text: str) -> str:
    """–ü—Ä–æ—Å—Ç–æ–π rule-based –∞–Ω–∞–ª–∏–∑ –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç"""
    text_lower = text.lower()
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ç–æ–Ω–∞
    neutral_words = [
        "–æ—Å—Ç–∞–ª", "—Å—Ç–∞–±–∏–ª—å–Ω", "–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω", "—Å–æ—Ö—Ä–∞–Ω", "–ø–æ–¥–¥–µ—Ä–∂", "—É–¥–µ—Ä–∂–∞–ª",
        "–Ω–∞ —É—Ä–æ–≤–Ω–µ", "–≤ —Ä–∞–º–∫–∞—Ö", "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤", "–ø–ª–∞–Ω–æ–≤", "–æ–∂–∏–¥–∞–Ω", "–ø—Ä–µ–∂–Ω", 
        "–Ω–µ–∏–∑–º–µ–Ω–Ω", "—Å—Ç–∞—Ç–∏—á–Ω–æ", "–ø–æ—Å—Ç–æ—è–Ω–Ω", "–æ–±—ã—á–Ω", "—Å—Ä–µ–¥–Ω", "—Ç–∏–ø–∏—á–Ω"
    ]
    
    # –£—Å–∏–ª–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞
    positive_words = [
        "—Ä–æ—Å—Ç", "–ø—Ä–∏–±—ã–ª—å", "—É—Å–ø–µ—Ö", "—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ", "—Ä–µ–∫–æ—Ä–¥", "–≤—ã–∏–≥—Ä", 
        "–ø–ª—é—Å", "–ø–æ–≤—ã—à", "—É–≤–µ–ª–∏—á", "—É–ª—É—á—à", "–ø–æ–∑–∏—Ç–∏–≤", "–≤—ã–≥–æ–¥", "–¥–æ—Ö–æ–¥",
        "–ø—Ä–µ–≤–æ—Å—Ö–æ–¥", "–¥–æ—Å—Ç–∏–∂", "breakthrough", "—Å–∫–∞—á–æ–∫", "–≤–∑–ª–µ—Ç", "–±—É–º"
    ]
    
    # –£—Å–∏–ª–µ–Ω–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞  
    negative_words = [
        "–ø–∞–¥–µ–Ω–∏–µ", "—É–±—ã—Ç–æ–∫", "–∫—Ä–∏–∑–∏—Å", "–ø–ª–æ—Ö–æ", "—É–∂–∞—Å–Ω–æ", "–ø—Ä–æ–≤–∞–ª", "–ø—Ä–æ–∏–≥—Ä",
        "–º–∏–Ω—É—Å", "—Å–Ω–∏–∑–∏–ª", "—É–º–µ–Ω—å—à", "—É—Ö—É–¥—à", "–Ω–µ–≥–∞—Ç–∏–≤", "–ø–æ—Ç–µ—Ä", "–¥–æ–ª–≥", "—É–ø–∞–ª",
        "–æ–±–≤–∞–ª", "–∫—Ä–∞—Ö", "–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ", "–∫–æ–ª–ª–∞–ø—Å", "—Å–ø–∞–¥", "—Ä–µ—Ü–µ—Å—Å"
    ]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å
    neutral_count = sum(1 for word in neutral_words if word in text_lower)
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–∏–ª—å–Ω—ã–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
    if neutral_count > 0 and pos_count == neg_count:
        return "neutral"
    elif pos_count > neg_count:
        return "positive"
    elif neg_count > pos_count:
        return "negative"
    else:
        return "neutral"

def classify_ru(text: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
    try:
        ru_tok, ru_mdl, ru_labels, _, _ = _load_models()
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
        if ru_tok is not None and ru_mdl is not None:
            inputs = ru_tok(text, return_tensors="pt", truncation=True, max_length=512)
            with torch.no_grad():
                logits = ru_mdl(**inputs).logits
            
            probabilities = torch.softmax(logits, dim=-1)
            predicted_idx = logits.argmax().item()
            predicted_label = ru_labels[predicted_idx]
            confidence = probabilities[0][predicted_idx].item()
            
            print(f"üîç RU MODEL: '{text[:50]}...'")
            print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {[f'{ru_labels[i]}={probabilities[0][i]:.3f}' for i in range(len(ru_labels))]}")
            print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
            
            result = _normalize_sentiment(predicted_label, "ru")
            
            # –í—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∞–µ–º rule-based —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            rule_result = _rule_based_sentiment_ru(text)
            print(f"‚ö° Rule-based —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {rule_result}")
            
            # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:
            # 1. –ï—Å–ª–∏ ML –æ—á–µ–Ω—å —É–≤–µ—Ä–µ–Ω (>0.85), –∏—Å–ø–æ–ª—å–∑—É–µ–º ML
            # 2. –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –º–µ–∂–¥—É ML –∏ rule-based, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            # 3. –ü—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ML (<0.65), –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º rule-based
            # 4. –ü—Ä–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–µ –∏ —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0.65-0.85), –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
            
            if confidence > 0.85:
                print(f"üéØ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ML ({confidence:.3f}), –∏—Å–ø–æ–ª—å–∑—É–µ–º: {result}")
            elif rule_result == result:
                print(f"ü§ù ML –∏ rule-based —Å–æ–≥–ª–∞—Å–Ω—ã: {result}")
            elif confidence < 0.65:
                print(f"üîÑ –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ML ({confidence:.3f}), –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based: {rule_result}")
                result = rule_result
            else:
                # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –ø—Ä–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–µ –≤ —Å—Ä–µ–¥–Ω–µ–π –∑–æ–Ω–µ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å
                if rule_result == "neutral" or result == "neutral":
                    result = "neutral"
                    print(f"‚öñÔ∏è –ö–æ–Ω—Ñ–ª–∏–∫—Ç –ø—Ä–∏ —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –≤—ã–±–∏—Ä–∞–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ: neutral")
                else:
                    print(f"ü§î –ö–æ–Ω—Ñ–ª–∏–∫—Ç: ML={result}, rule={rule_result}, –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based")
                    result = rule_result
            
            print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            return result
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based
        else:
            print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º rule-based –∞–Ω–∞–ª–∏–∑ –¥–ª—è: '{text[:50]}...'")
            result = _rule_based_sentiment_ru(text)
            print(f"‚úÖ Rule-based —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            return result
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π rule-based –∞–Ω–∞–ª–∏–∑
        return _rule_based_sentiment_ru(text)

def classify_en(text: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    try:
        _, _, _, en_tok, en_mdl = _load_models()
        if en_tok is None or en_mdl is None:
            return "neutral"
        
        inputs = en_tok(text, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            logits = en_mdl(**inputs).logits
        
        probabilities = torch.softmax(logits, dim=-1)
        predicted_idx = logits.argmax().item()
        predicted_label = EN_LABELS[predicted_idx]
        confidence = probabilities[0][predicted_idx].item()
        
        print(f"üîç EN DEBUG: '{text[:50]}...'")
        print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {[f'{EN_LABELS[i]}={probabilities[0][i]:.3f}' for i in range(len(EN_LABELS))]}")
        print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
        
        result = _normalize_sentiment(predicted_label, "en")
        print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        return result
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
        return "neutral"

def classify_multi(text: str) -> str:
    """–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è"""
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
        lang = detect(text[:200])
        if lang == "ru":
            return classify_ru(text)
        else:
            return classify_en(text)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞: {e}")
        # –ï—Å–ª–∏ —è–∑—ã–∫ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª—Å—è, –ø—Ä–æ–±—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
        return classify_en(text)

# –°—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def classify(text: str) -> str:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - –∞–Ω–∞–ª–∏–∑ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    return classify_ru(text)

# --- RSS-–≥—Ä–∞–±–±–µ—Ä —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ ---
RSS_FEEDS = [
    "https://tass.ru/rss/v2.xml",
    "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
]

def latest_news_ru(ticker: str, hours: int = 24) -> list[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä—É—Å—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS"""
    cutoff = dt.datetime.utcnow() - dt.timedelta(hours=hours)
    found = []
    for url in RSS_FEEDS:
        try:
            xml = requests.get(url, timeout=5).text
            for it in xml.split("<item>")[1:]:
                title = it.split("<title>")[1].split("</title>")[0]
                pub   = it.split("<pubDate>")[1].split("</pubDate>")[0]
                dt_pub = dt.datetime.strptime(pub[:-6], "%a, %d %b %Y %H:%M:%S")
                if dt_pub > cutoff and ticker.lower() in title.lower():
                    found.append(title)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS {url}: {e}")
            continue
    return found

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def latest_news(ticker: str, hours: int = 24) -> list[str]:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - —Ä—É—Å—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    return latest_news_ru(ticker, hours)

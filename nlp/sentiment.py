from functools import lru_cache
# ‚¨á –æ—Ç–∫–ª—é—á–µ–Ω–æ: from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from news_feed import fetch_news
from nlp.news_rss_async import async_fetch_all
import asyncio
from langdetect import detect
import warnings
from typing import Dict, List, Tuple, Optional
import statistics

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤–µ—Å–∞—Ö –º–æ–¥–µ–ª–∏
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è ensemble
MODEL_CONFIG = {
    "ru_models": [
        {
            "name": "seara/rubert-base-cased-russian-sentiment",
            "labels": ["NEGATIVE", "NEUTRAL", "POSITIVE"],
            "weight": 0.3,
            "description": "RuBERT —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π"
        },
        {
            "name": "blanchefort/rubert-base-cased-sentiment",
            "labels": ["NEUTRAL", "POSITIVE", "NEGATIVE"],
            "weight": 0.25,
            "description": "RuBERT –±–∞–∑–æ–≤—ã–π"
        },
        {
            "name": "nlptown/bert-base-multilingual-uncased-sentiment",
            "labels": ["1", "2", "3", "4", "5"],  # 1-2=negative, 3=neutral, 4-5=positive
            "weight": 0.45,
            "description": "–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è)"
        }
    ],
    "en_models": [
        {
            "name": "ProsusAI/finbert",
            "labels": ["positive", "negative", "neutral"],
            "weight": 0.6,
            "description": "FinBERT —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤"
        },
        {
            "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
            "labels": ["LABEL_0", "LABEL_1", "LABEL_2"],
            "weight": 0.4,
            "description": "Twitter RoBERTa"
        }
    ]
}

class FinancialSentimentEnsemble:
    """Ensemble –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ª–æ–≥–∏–∫–æ–π"""

    def __init__(self):
        # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å–ª–æ–≤–∞—Ä–∏
        self.financial_terms = {
            'strong_positive': {
                'ru': ['—Ä–µ–∫–æ—Ä–¥', '–≤–∑–ª–µ—Ç', '—Å–∫–∞—á–æ–∫', '–±—É–º', '–ø—Ä–µ–≤–∑–æ—à', '–ø—Ä–æ—Ä—ã–≤', '—Ä–µ–∑–∫–∏–π —Ä–æ—Å—Ç'],
                'en': ['breakthrough', 'surge', 'soar', 'rally', 'boom', 'outperform', 'beat']
            },
            'moderate_positive': {
                'ru': ['–≤—ã—Ä–æ—Å–ª–∏', '—Ä–æ—Å—Ç', '—É–≤–µ–ª–∏—á', '–ø–æ–≤—ã—à', '—É–ª—É—á—à', '–ø—Ä–∏–±—ã–ª—å', '–¥–æ—Ö–æ–¥'],
                'en': ['improved', 'gained', 'rise', 'increase', 'profit', 'earnings', 'revenue']
            },
            'strong_negative': {
                'ru': ['–æ–±–≤–∞–ª', '–∫—Ä–∞—Ö', '–∫—Ä–∏–∑–∏—Å', '–∫–æ–ª–ª–∞–ø—Å', '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ', '–ø—Ä–æ–≤–∞–ª'],
                'en': ['plummet', 'crash', 'collapse', 'crisis', 'catastrophe', 'disaster']
            },
            'moderate_negative': {
                'ru': ['—É–ø–∞–ª–∏', '—Å–Ω–∏–∑–∏–ª', '–ø–∞–¥–µ–Ω–∏–µ', '—É–º–µ–Ω—å—à', '—É–±—ã—Ç', '–ø–æ—Ç–µ—Ä'],
                'en': ['declined', 'dropped', 'fell', 'loss', 'decrease', 'down']
            },
            'neutral_stable': {
                'ru': ['—Å—Ç–∞–±–∏–ª—å–Ω', '–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω', '–æ—Å—Ç–∞–ª', '–Ω–µ–∏–∑–º–µ–Ω–Ω'],
                'en': ['remained', 'stable', 'flat', 'unchanged', 'steady']
            }
        }

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        self.number_pattern = r'(\d+(?:,\d+)?(?:\.\d+)?)\s*%'

        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤
        self.weights = {
            'ml_ensemble': 0.5,      # –í–µ—Å ML-–∞–Ω—Å–∞–º–±–ª—è
            'financial_terms': 0.3,   # –í–µ—Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            'numeric_context': 0.2    # –í–µ—Å —á–∏—Å–ª–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        }

    def _normalize_multilingual_sentiment(self, label: str, model_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–µ—Ç–∫–∏ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        if "nlptown" in model_name:
            # –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å: 1-2=negative, 3=neutral, 4-5=positive
            if label in ["1", "2"]:
                return "negative"
            elif label == "3":
                return "neutral"
            elif label in ["4", "5"]:
                return "positive"
        elif "finbert" in model_name.lower():
            # FinBERT —É–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏
            return label.lower()
        elif "twitter" in model_name or "cardiff" in model_name:
            # Twitter RoBERTa
            mapping = {"LABEL_0": "negative", "LABEL_1": "neutral", "LABEL_2": "positive"}
            return mapping.get(label, "neutral")
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ RuBERT –º–æ–¥–µ–ª–∏
            label_upper = label.upper()
            if "NEGATIVE" in label_upper or "NEG" in label_upper:
                return "negative"
            elif "POSITIVE" in label_upper or "POS" in label_upper:
                return "positive"
            else:
                return "neutral"

    def _extract_financial_signals(self, text: str, lang: str) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        signals = {'positive': 0, 'negative': 0, 'neutral': 0}

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        for sentiment_type, terms_dict in self.financial_terms.items():
            if lang in terms_dict:
                for term in terms_dict[lang]:
                    if term in text_lower:
                        if 'positive' in sentiment_type:
                            weight = 2.0 if 'strong' in sentiment_type else 1.0
                            signals['positive'] += weight
                        elif 'negative' in sentiment_type:
                            weight = 2.0 if 'strong' in sentiment_type else 1.0
                            signals['negative'] += weight
                        else:  # neutral
                            signals['neutral'] += 1.0

        return signals

    def _extract_numeric_context(self, text: str) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á–∏—Å–ª–æ–≤–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø—Ä–æ—Ü–µ–Ω—Ç—ã, —Å—É–º–º—ã)"""
        context = {'magnitude': 0, 'direction': 0}  # direction: +1=—Ä–æ—Å—Ç, -1=–ø–∞–¥–µ–Ω–∏–µ

        # –ò—â–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        numbers = re.findall(self.number_pattern, text.lower())
        if numbers:
            try:
                max_number = max(float(num.replace(',', '.')) for num in numbers)
                context['magnitude'] = max_number

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                growth_words = ['–≤—ã—Ä–æ—Å–ª–∏', '—Ä–æ—Å—Ç', '—É–≤–µ–ª–∏—á', '–ø–æ–≤—ã—à', 'gained', 'rise', 'up']
                decline_words = ['—É–ø–∞–ª–∏', '—Å–Ω–∏–∑–∏–ª', '–ø–∞–¥–µ–Ω–∏–µ', '—É–º–µ–Ω—å—à', 'declined', 'dropped', 'down']

                text_lower = text.lower()
                if any(word in text_lower for word in growth_words):
                    context['direction'] = 1
                elif any(word in text_lower for word in decline_words):
                    context['direction'] = -1

            except ValueError:
                pass

        return context

    def _ensemble_predict(self, text: str, models_config: List[Dict], lang: str = "ru") -> Dict[str, float]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç ensemble –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
        predictions = []
        total_weight = 0

        for model_info in models_config:
            try:
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å
                tokenizer = AutoTokenizer.from_pretrained(model_info["name"])
                model = AutoModelForSequenceClassification.from_pretrained(model_info["name"])
                model.eval()

                inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    logits = model(**inputs).logits

                probabilities = torch.softmax(logits, dim=-1)
                predicted_idx = logits.argmax().item()
                predicted_label = model_info["labels"][predicted_idx]
                confidence = probabilities[0][predicted_idx].item()

                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                normalized_sentiment = self._normalize_multilingual_sentiment(predicted_label, model_info["name"])

                predictions.append({
                    'sentiment': normalized_sentiment,
                    'confidence': confidence,
                    'weight': model_info["weight"],
                    'model': model_info["description"]
                })
                total_weight += model_info["weight"]

                print(f"ü§ñ {model_info['description']}: {normalized_sentiment} (conf: {confidence:.3f})")

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ {model_info['name']}: {e}")
                continue

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if not predictions:
            return {'sentiment': 'neutral', 'confidence': 0.0, 'details': []}

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        weighted_scores = {'positive': 0, 'negative': 0, 'neutral': 0}
        total_confidence = 0

        for pred in predictions:
            weight_norm = pred['weight'] / total_weight
            weighted_scores[pred['sentiment']] += weight_norm * pred['confidence']
            total_confidence += weight_norm * pred['confidence']

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        final_sentiment = max(weighted_scores, key=weighted_scores.get)
        final_confidence = total_confidence / len(predictions)

        return {
            'sentiment': final_sentiment,
            'confidence': final_confidence,
            'details': predictions,
            'scores': weighted_scores
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä ensemble –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
_ensemble_analyzer = FinancialSentimentEnsemble()

@lru_cache(maxsize=10)
def _load_ensemble_models():
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è ensemble"""
    print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ensemble –º–æ–¥–µ–ª–µ–π...")
    return True  # –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤ _ensemble_predict

@lru_cache(maxsize=256)
def classify_ru_ensemble(text: str) -> str:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é ensemble –º–æ–¥–µ–ª–µ–π"""
    return _ensemble_classify(text, MODEL_CONFIG["ru_models"])

@lru_cache(maxsize=256)  
def classify_en_ensemble(text: str) -> str:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é ensemble –º–æ–¥–µ–ª–µ–π"""
    return _ensemble_classify(text, MODEL_CONFIG["en_models"])

def _extract_financial_signals(text: str) -> Dict[str, float]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–≥–ª–æ–±–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è)"""
    financial_terms = {
        'strong_positive': {
            'ru': ['—Ä–µ–∫–æ—Ä–¥', '–≤–∑–ª–µ—Ç', '—Å–∫–∞—á–æ–∫', '–±—É–º', '–ø—Ä–µ–≤–∑–æ—à', '–ø—Ä–æ—Ä—ã–≤', '—Ä–µ–∑–∫–∏–π —Ä–æ—Å—Ç'],
            'en': ['breakthrough', 'surge', 'soar', 'rally', 'boom', 'outperform', 'beat']
        },
        'moderate_positive': {
            'ru': ['–≤—ã—Ä–æ—Å–ª–∏', '—Ä–æ—Å—Ç', '—É–≤–µ–ª–∏—á', '–ø–æ–≤—ã—à', '—É–ª—É—á—à', '–ø—Ä–∏–±—ã–ª—å', '–¥–æ—Ö–æ–¥'],
            'en': ['improved', 'gained', 'rise', 'increase', 'profit', 'earnings', 'revenue']
        },
        'strong_negative': {
            'ru': ['–æ–±–≤–∞–ª', '–∫—Ä–∞—Ö', '–∫—Ä–∏–∑–∏—Å', '–∫–æ–ª–ª–∞–ø—Å', '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ', '–ø—Ä–æ–≤–∞–ª'],
            'en': ['plummet', 'crash', 'collapse', 'crisis', 'catastrophe', 'disaster']
        },
        'moderate_negative': {
            'ru': ['—É–ø–∞–ª–∏', '—Å–Ω–∏–∑–∏–ª', '–ø–∞–¥–µ–Ω–∏–µ', '—É–º–µ–Ω—å—à', '—É–±—ã—Ç', '–ø–æ—Ç–µ—Ä'],
            'en': ['declined', 'dropped', 'fell', 'loss', 'decrease', 'down']
        },
        'neutral_stable': {
            'ru': ['—Å—Ç–∞–±–∏–ª—å–Ω', '–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω', '–æ—Å—Ç–∞–ª', '–Ω–µ–∏–∑–º–µ–Ω–Ω'],
            'en': ['remained', 'stable', 'flat', 'unchanged', 'steady']
        }
    }

    text_lower = text.lower()
    signals = {'positive': 0, 'negative': 0, 'neutral': 0}

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
    lang = 'ru' if any(char in '–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è' for char in text_lower[:50]) else 'en'

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    for sentiment_type, terms_dict in financial_terms.items():
        if lang in terms_dict:
            for term in terms_dict[lang]:
                if term in text_lower:
                    if 'positive' in sentiment_type:
                        weight = 2.0 if 'strong' in sentiment_type else 1.0
                        signals['positive'] += weight
                    elif 'negative' in sentiment_type:
                        weight = 2.0 if 'strong' in sentiment_type else 1.0
                        signals['negative'] += weight
                    else:  # neutral
                        signals['neutral'] += 1.0

    return signals

def _ensemble_classify(text: str, models_config: list) -> str:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç ensemble –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    predictions = []
    total_weight = 0

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
    active_models = models_config[:2]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 2 –º–æ–¥–µ–ª–∏

    for model_info in active_models:
        try:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            tokenizer = AutoTokenizer.from_pretrained(
                model_info["name"], 
                cache_dir=".model_cache",
                local_files_only=False
            )
            model = AutoModelForSequenceClassification.from_pretrained(
                model_info["name"],
                cache_dir=".model_cache", 
                local_files_only=False
            )
            model.eval()

            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            with torch.no_grad():
                logits = model(**inputs).logits

            probabilities = torch.softmax(logits, dim=-1)
            predicted_idx = logits.argmax().item()
            predicted_label = model_info["labels"][predicted_idx]
            confidence = probabilities[0][predicted_idx].item()

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            if predicted_label.upper() in ["POSITIVE", "POS"]:
                sentiment_score = 1.0
            elif predicted_label.upper() in ["NEGATIVE", "NEG"]:
                sentiment_score = -1.0
            else:
                sentiment_score = 0.0

            predictions.append(sentiment_score * confidence * model_info["weight"])
            total_weight += model_info["weight"]

        except Exception as e:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_info['name'][:20]}... –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            continue

    if not predictions:
        return "neutral"

    # –í—ã—á–∏—Å–ª—è–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    ensemble_score = sum(predictions) / total_weight if total_weight > 0 else 0

    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    financial_signals = _extract_financial_signals(text)

    # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä —Å —É—á–µ—Ç–æ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
    final_score = (
        ensemble_score * 0.8 +
        (financial_signals['positive'] - financial_signals['negative']) * 0.2
    )

    if final_score > 0.1:
        result = "positive"
    elif final_score < -0.1:
        result = "negative"
    else:
        result = "neutral"

    return result

# –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å ensemble –ø–æ–¥—Ö–æ–¥–æ–º
def classify_ru(text: str) -> str:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    return classify_ru_ensemble(text)

def classify_en(text: str) -> str:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    return classify_en_ensemble(text)

def classify_multi(text: str) -> str:
    """–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Å ensemble"""
    try:
        lang = detect(text[:200])
        if lang == "ru":
            return classify_ru_ensemble(text)
        else:
            return classify_en_ensemble(text)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞: {e}")
        return classify_en_ensemble(text)

def analyze_sentiment_trend(texts: List[str]) -> Dict[str, float]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É —Ç–µ–∫—Å—Ç–æ–≤"""
    if not texts:
        return {'trend': 0.0, 'confidence': 0.0, 'count': 0}

    sentiments = []
    for text in texts:
        sentiment = classify_multi(text)
        score = {'positive': 1, 'negative': -1, 'neutral': 0}.get(sentiment, 0)
        sentiments.append(score)

    avg_sentiment = sum(sentiments) / len(sentiments)
    consistency = 1.0 - (len(set(sentiments)) - 1) / 2.0

    return {
        'trend': avg_sentiment,
        'confidence': consistency,
        'count': len(texts),
        'distribution': {
            'positive': sentiments.count(1),
            'negative': sentiments.count(-1),
            'neutral': sentiments.count(0)
        }
    }

# –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
def classify(text: str) -> str:
    return classify_ru(text)

def classify_llm(text: str) -> str:
    """–ê–ª–∏–∞—Å –¥–ª—è LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ sentiment_llm –º–æ–¥—É–ª—å"""
    from nlp.sentiment_llm import smart_classify
    return smart_classify(text)
# ------------------------------------------------------------------------
# ‚Üì‚Üì‚Üì MINI-RSS helper: üá∑üá∫-–∑–∞–≥–æ–ª–æ–≤–∫–∏ –∑–∞ N —á–∞—Å–æ–≤
# ------------------------------------------------------------------------
from datetime import datetime, timedelta
from nlp.news_feed import _rss_query

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  RSS-Sentiment summary (OpenAI)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os, json, openai
from collections import Counter
from typing import Dict

def rss_sentiment_summary(hours: int = 24) -> Dict[str, int]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {'positive': N, 'neutral': M, 'negative': K} –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö RSS-–Ω–æ–≤–æ—Å—Ç–µ–π
    –∑–∞ *hours* —á–∞—Å–æ–≤.  –ú–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –ø—É—Å—Ç–æ–π dict, –µ—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç.
    –¢—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY.
    """
    headlines = fetch_ru_news(hours)
    if not headlines:
        return {}

    openai.api_key = os.getenv("OPENAI_API_KEY")
    if not openai.api_key:
        raise RuntimeError("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    prompt = (
        "–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (positive / neutral / negative) "
        "–∏ –≤—ã–≤–µ–¥–∏ JSON –≤–∏–¥–∞ {\"positive\":N,\"neutral\":M,\"negative\":K}.\n\n"
        + "\n".join(f"- {h}" for h in headlines[:25])  # ‚â§25 —Å—Ç—Ä–æ–∫, —á—Ç–æ–±—ã —É–ª–æ–∂–∏—Ç—å—Å—è –≤ —Ç–æ–∫–µ–Ω—ã
    )

    chat = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=50,
        temperature=0,
    )
    try:
        counts = json.loads(chat.choices[0].message.content.strip())
        # –ø—Ä–∏–≤–æ–¥–∏–º –∫ int –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏ –Ω—É–ª—è–º–∏
        total = Counter({k: int(counts.get(k, 0)) for k in ("positive", "neutral", "negative")})
        return dict(total)
    except Exception:
        return {}


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üöÄ robust fetch_ru_news ‚Äî –±–µ—Ä—ë—Ç –≤—Å–µ RSS-–ª–µ–Ω—Ç—ã, –≥–¥–µ –±—ã –æ–Ω–∏ –Ω–∏ —Ö—Ä–∞–Ω–∏–ª–∏—Å—å
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from datetime import datetime, timedelta


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üöÄ robust fetch_ru_news ‚Äî —Å–æ–±–∏—Ä–∞–µ—Ç üá∑üá∫ RSS-–∑–∞–≥–æ–ª–æ–≤–∫–∏ –∑–∞ N —á–∞—Å–æ–≤
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from datetime import datetime, timedelta

    cutoff  = datetime.utcnow() - timedelta(hours=hours)
    titles: list[str] = []

    for url in _FEEDS:
        try:
            for art in _rss_query(url):
                if (
                    art.get("dt") and art["dt"] >= cutoff
                    and art.get("title")
                ):
                    titles.append(art["title"].strip())
        except Exception:
            # –ø–∞–¥–∞—é—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            continue
    return titles


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üöÄ robust fetch_ru_news ‚Äî —Å–æ–±–∏—Ä–∞–µ—Ç üá∑üá∫ RSS-–∑–∞–≥–æ–ª–æ–≤–∫–∏ –∑–∞ N —á–∞—Å–æ–≤
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from datetime import datetime, timedelta
def fetch_ru_news(hours: int = 24) -> list[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∑–∞ *hours* —á–∞—Å–æ–≤."""
    # ‚ûä –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–ø–∏—Å–æ–∫ –ª–µ–Ω—Ç
    try:
        from nlp.news_feed import RSS_FEED_URLS as _FEEDS            # —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
    except ImportError:
        try:
            from nlp.news_rss_async import RSS_FEEDS as _DICT        # —Å—Ç–∞—Ä—ã–π —Å–ª–æ–≤–∞—Ä—å
            _FEEDS = list(_DICT.values())
        except ImportError:
            _FEEDS = []                                              # –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏

    # ‚ûã –±–µ—Ä—ë–º –ø–∞—Ä—Å–µ—Ä RSS-–ª–µ–Ω—Ç—ã
    try:
        from nlp.news_feed import _rss_query
    except ImportError:
        return []

    cutoff  = datetime.utcnow() - timedelta(hours=hours)
    titles: list[str] = []

    for url in _FEEDS:
        try:
            for art in _rss_query(url):
                if art.get("dt") and art["dt"] >= cutoff and art.get("title"):
                    titles.append(art["title"].strip())
        except Exception:
            continue    # –ø–∞–¥–∞—é—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    return titles


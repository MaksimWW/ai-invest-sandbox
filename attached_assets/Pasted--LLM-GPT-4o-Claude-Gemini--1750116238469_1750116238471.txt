# ЦЕЛЬ: перейти на чистый LLM-анализ настроения (GPT-4o / Claude / Gemini),
# полностью убрав локальные BERT-модели. Нужен план действий и тестирование,
# без прямого кода.

────────────────────────────────────────────────────────────────────────────
I. ОБЩИЙ ПЛАН
────────────────────────────────────────────────────────────────────────────
1. Архитектура
   • Один классификатор:  LLM-API  (по умолчанию GPT-4o).
   • Redis-кэш на 24 ч → минимизирует расходы.
   • Функция  smart_classify(text)  выдаёт label ∈ {positive, neutral, negative}.

2. Workflow
   2.1  RSS / NewsAPI / GDELT  → очередь заголовков.  
   2.2  Для каждого заголовка:  
        ─ проверяем кэш → если нет — отправляем в LLM.  
   2.3  Сохраняем label и timestamp в SQLite `news_cache.db`.  
   2.4  get_sentiment_score(ticker, hours) читает только из SQLite  
        (никаких вызовов моделей во время команды /ideas).

3. Переменные окружения
   OPENAI_API_KEY          – ключ GPT-4o  
   LLM_TEMP  = 0.0         – температура  
   LLM_MAXTOK = 8          – экономия токенов  
   CACHE_HOURS = 24        – TTL кэша  
   LLM_OFF     = 0/1       – флаг аварийного отключения

────────────────────────────────────────────────────────────────────────────
II. ТОЧЕЧНЫЕ ШАГИ ДЛЯ ВНЕДРЕНИЯ
────────────────────────────────────────────────────────────────────────────
1. **Удалить** все упоминания RuBERT / FinBERT из requirements и кода.

2. Создать модуль `sentiment_llm.py`
   • build_prompt(text) – system + user, max 15 слов.  
   • async call_openai(prompt, key, max_tok=8, temp=LLM_TEMP).  
   • smart_classify(text) – проверяет Redis-кэш, при miss вызывает LLM.  
   • Хранит label (+prob, если logprobs доступны) в Redis и SQLite.

3. Пакеты для requirements.txt
   openai>=1.25  
   redis>=5.0  
   aiosqlite>=0.19

4. Обновить `get_sentiment_score`  
   • Берёт все записи из SQLite между now-hours и now,  
     фильтрует по ticker (case-insensitive),  
     суммирует positive (+1), negative (–1), neutral (0).

5. Команда `/ideas`
   • Считает score = Technical(±1) + Sentiment(–1/0/+1).  
   • Порог оставляем |score| ≥ 2.

────────────────────────────────────────────────────────────────────────────
III. ПЛАН ТЕСТИРОВАНИЯ
────────────────────────────────────────────────────────────────────────────
A. Юнит-тесты (`tests/test_sentiment_llm.py`)
   • mock OpenAI → возвращает “positive” / “negative”.  
   • Проверить:  smart_classify  кэширует результат,  
     второй вызов не бьёт openai.create_chat_completion.  

B. Rate-limit тест  
   • 1 000 случайных заголовков, параллельно 5 воркеров.  
   • Ожидаем ≤ 1 000 токенов API, Redis hit-rate ≥ 80 % после двух прогонов.

C. Интеграция  
   • Запустить `/ideas 5 15 0 24 YNDX`  
   • В логах видеть `[LLM] 15 requests, 200 cached`.

D. Расходы  
   • Счётчик `llm_used_tokens_daily` в Redis.  
   • alert > 100 k токенов → вывод в консоль «LLM_OFF=1».

────────────────────────────────────────────────────────────────────────────
IV. ПОСЛЕ ВНЕДРЕНИЯ
────────────────────────────────────────────────────────────────────────────
• README: раздел «Sentiment v3 – only LLM + Redis cache».  
• Удалить старый модуль `sentiment.py`, все BERT-веса.  
• Проверка latency:  → цель 300–600 мс/заголовок при cache-miss.

# Конец промпта